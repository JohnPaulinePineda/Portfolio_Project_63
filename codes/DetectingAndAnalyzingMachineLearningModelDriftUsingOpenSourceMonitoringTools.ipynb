{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e29c65d",
   "metadata": {},
   "source": [
    "***\n",
    "# Model Deployment : Detecting and Analyzing Machine Learning Model Drift Using Open-Source Monitoring Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52d179",
   "metadata": {},
   "source": [
    "***\n",
    "### [**John Pauline Pineda**](https://github.com/JohnPaulinePineda) <br> <br> *October 15, 2025*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced1d2b",
   "metadata": {},
   "source": [
    "* [**1. Table of Contents**](#TOC)\n",
    "    * [1.1 Data Background](#1.1)\n",
    "    * [1.2 Data Description](#1.2)\n",
    "    * [1.3 Data Quality Assessment](#1.3)\n",
    "    * [1.4 Data Preprocessing](#1.4)\n",
    "        * [1.4.1 Data Splitting](#1.4.1)\n",
    "        * [1.4.2 Outlier and Distributional Shape Analysis](#1.4.4)\n",
    "        * [1.4.3 Collinearity](#1.4.5)\n",
    "    * [1.5 Data Exploration](#1.5)\n",
    "        * [1.5.1 Exploratory Data Analysis](#1.5.1)\n",
    "        * [1.5.2 Hypothesis Testing](#1.5.2)\n",
    "    * [1.6 Premodelling Data Preparation](#1.6)\n",
    "        * [1.6.1 Preprocessed Data Description](#1.6.1)\n",
    "        * [1.6.2 Preprocessing Pipeline Development](#1.6.2)\n",
    "    * [1.7 Model Development and Validation](#1.7)\n",
    "        * [1.7.1 Random Forest](#1.7.1)\n",
    "        * [1.7.2 AdaBoost](#1.7.2)\n",
    "        * [1.7.3 Gradient Boosting](#1.7.3)\n",
    "        * [1.7.4 XGBoost](#1.7.4)\n",
    "        * [1.7.5 Light GBM](#1.7.5)\n",
    "        * [1.7.6 CatBoost](#1.7.6)\n",
    "    * [1.8 Model Monitoring using the NannyML Framework](#1.8)\n",
    "        * [1.8.1 Baseline Control](#1.8.1)\n",
    "        * [1.8.2 Simulated Covariate Drift](#1.8.2)\n",
    "        * [1.8.3 Simulated Prior Shift](#1.8.3)\n",
    "        * [1.8.4 Simulated Concept Drift](#1.8.4)\n",
    "        * [1.8.5 Simulated Missingness Spike](#1.8.5)\n",
    "        * [1.8.6 Simulated Seasonal Pattern](#1.8.6)\n",
    "* [**2. Summary**](#Summary)   \n",
    "* [**3. References**](#References)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc698d",
   "metadata": {},
   "source": [
    "# 1. Table of Contents <a class=\"anchor\" id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde58b8",
   "metadata": {},
   "source": [
    "This project investigates **open-source frameworks for post-deployment model monitoring and performance estimation**, with a particular focus on **NannyML** or detecting and interpreting shifts in machine learning pipelines using <mark style=\"background-color: #CCECFF\"><b>Python</b></mark>. The objective was to systematically analyze how different types of drift and distribution changes manifest after model deployment, and to demonstrate how robust monitoring mitigates risks of performance degradation and biased decision-making. The workflow began with the development and selection of a baseline predictive model, which serves as a reference for stability. The dataset was then deliberately perturbed to simulate a range of realistic post-deployment scenarios: **Covariate Drift** (shifts in feature distributions), **Prior Shift** (changes in target label proportions), **Concept Drift** (evolving relationships between features and outcomes), **Missingness Spikes** (abrupt increases in absent data), and **Seasonal Patterns** (periodic variations in distributions). NannyML’s statistical tests, visualization capabilities, and performance estimation methods were subsequently applied to diagnose these shifts, evaluate their potential impact, and provide interpretable insights into model reliability. By contrasting baseline and perturbed conditions, the experiment demonstrated how continuous monitoring augments traditional offline evaluation, offering a safeguard against hidden risks. The findings highlighted how tools like NannyML can integrate seamlessly into MLOps workflows to enable proactive governance, early warning systems, and sustainable deployment practices. All results were consolidated in a [<span style=\"color: #FF0000\"><b>Summary</b></span>](#Summary) presented at the end of the document.\n",
    "\n",
    "[Post-Deployment Monitoring](https://www.nannyml.com/) refers to the continuous oversight of machine learning models once they are integrated into production systems. Unlike offline evaluation, which relies on static validation datasets, monitoring addresses the challenges of evolving real-world data streams where underlying distributions may shift. Effective monitoring ensures that models remain accurate, unbiased, and aligned with business objectives. In MLOps, monitoring encompasses data integrity checks, drift detection, performance estimation, and alerting mechanisms. NannyML operationalizes this concept by focusing on performance estimation without ground truth, and by offering statistical methods to detect when data or predictions deviate from expected baselines. The challenges of post-deployment monitoring include delayed or missing ground truth labels, non-stationary data, hidden feedback loops, and difficulties distinguishing natural fluctuations from problematic drifts. Common solutions involve deploying drift detection algorithms, conducting regular audits of data pipelines, simulating counterfactuals, and retraining models on updated data. Monitoring frameworks must balance sensitivity (detecting real problems quickly) with robustness (avoiding false alarms caused by natural noise). Another key challenge is explainability: stakeholders need interpretable signals that justify interventions such as retraining or rolling back models. Tools like NannyML address these challenges through statistical tests for data drift, performance estimation without labels, missingness tracking, and visual diagnostics, making monitoring actionable for data scientists and business teams alike.\n",
    "\n",
    "[Covariate Shift](https://www.nannyml.com/) occurs when the distribution of input features changes over time compared to the data used to train the model. Also known as data drift, it does not necessarily imply that the model’s predictive mapping is invalid, but it often precedes performance degradation. Detecting covariate drift requires comparing feature distributions between baseline (reference) data and incoming production data. NannyML provides multiple statistical tests and visualization tools to flag significant changes. Key signatures of covariate shift include shifts in summary statistics (mean, variance), changes in distributional shape, or increased divergence between reference and production feature distributions. These shifts may lead to poor generalization, as the model has not been exposed to the altered feature ranges. Detection techniques include univariate statistical tests (e.g., Kolmogorov–Smirnov, Chi-square), multivariate distance measures (e.g., Jensen–Shannon divergence, Population Stability Index), and density estimation methods. Remediation approaches involve domain adaptation, re-weighting training samples, or retraining models on updated data distributions. NannyML implements univariate and multivariate tests, provides drift magnitude quantification, and visualizes feature-level changes, allowing practitioners to pinpoint which features are most responsible for the detected drift.\n",
    "\n",
    "[Prior Shift](https://www.nannyml.com/) arises when the distribution of the target variable changes, while the conditional relationship between features and labels remains stable. This is also referred to as label shift. Models trained on the original distribution may underperform because their predictions no longer match the new class priors. Detecting prior shifts is crucial, especially in imbalanced classification tasks where small changes in priors can lead to large performance impacts. Prior shift is typically characterized by systematic increases or decreases in class frequencies without corresponding changes in feature distributions. Its impact includes skewed decision thresholds, inflated false positives/negatives, and degraded calibration of predicted probabilities. Detection approaches include monitoring predicted class proportions, estimating priors using EM-based algorithms, and re-weighting predictions to align with new distributions. Correction strategies may involve resampling, threshold adjustment, or cost-sensitive learning. NannyML assists by tracking predicted probability distributions and comparing them against reference priors, using techniques such as KL divergence and PSI to quantify the magnitude of shift.\n",
    "\n",
    "[Concept Drift](https://www.nannyml.com/) occurs when the underlying relationship between input features and target labels evolves over time. Unlike covariate shift, where features change independently, concept drift implies that the model’s mapping function itself becomes outdated. Concept drift is among the most damaging forms of drift because it directly undermines predictive accuracy. Detecting it often requires monitoring model outputs or inferred performance over time. NannyML addresses this by estimating performance even when ground truth labels are unavailable. Concept drift is typically signaled by a gradual or sudden decline in performance metrics, inconsistent error patterns, or misalignment between expected and actual prediction behavior. Its impact is severe: models may lose predictive power entirely if they cannot adapt. Detection methods include window-based performance monitoring, hypothesis testing, adaptive ensembles, and statistical monitoring of residuals. Corrective actions include periodic retraining, incremental learning, and online adaptation strategies. NannyML leverages Confidence-Based Performance Estimation (CBPE) and other statistical techniques to estimate performance degradation without labels, making it possible to detect concept drift in real-time production environments.\n",
    "\n",
    "[Missingness Spike](https://www.nannyml.com/) refers to sudden increases in missing values within production data. Missing features can destabilize preprocessing pipelines, distort predictions, and signal upstream data collection failures. Monitoring missingness is critical for ensuring both model reliability and data pipeline health. NannyML provides built-in mechanisms to track and visualize changes in missing data patterns, alerting stakeholders before downstream impacts occur. Key indicators of missingness spikes include abrupt rises in null counts, missing categorical levels, or structural breaks in feature completeness. The consequences range from biased predictions to outright system failures if preprocessing pipelines cannot handle unexpected missingness. Detection methods include statistical monitoring of missing value proportions, anomaly detection on completeness metrics, and threshold-based alerts. Solutions typically involve robust imputation, pipeline hardening, and upstream data validation. NannyML offers automated missingness detection, completeness trend visualization, and configurable thresholds, ensuring that missingness issues are surfaced early.\n",
    "\n",
    "[Seasonal Pattern Shift](https://www.nannyml.com/) represents periodic fluctuations in data distributions or outcomes that follow predictable cycles. If models are not trained with sufficient historical data to capture these patterns, their predictions may systematically underperform during certain periods. NannyML’s monitoring can reveal recurring deviations, helping teams distinguish between natural seasonality and genuine drift that requires retraining. Seasonality is often characterized by cyclic patterns in data features, prediction distributions, or performance metrics. Its impact includes systematic biases, recurring error peaks, and difficulty distinguishing drift from natural variability. Detection techniques include autocorrelation analysis, Fourier decomposition, and seasonal-trend decomposition. Mitigation strategies involve training with longer historical datasets, adding time-related features, or developing seasonally adaptive models. NannyML highlights recurring deviations in drift metrics, making it easier for practitioners to separate cyclical behavior from true degradation, ensuring that alerts are contextually relevant.\n",
    "\n",
    "[Performance Estimation Without Labels](https://www.nannyml.com/) refers to scenarios in real-world deployments where the ground truth often arrives with delays—or may never be available. This makes direct performance tracking difficult. NannyML addresses this challenge by providing algorithms to estimate model performance without labels using confidence distributions, statistical inference, and robust estimation techniques. This capability allows practitioners to maintain visibility into model health continuously, even in label-scarce settings, bridging a critical gap in MLOps monitoring practices. Algorithms in this domain include Confidence-Based Performance Estimation (CBPE), which infers performance by comparing predicted probability distributions against expected confidence intervals, and Direct Loss Estimation, which approximates error rates based on calibration. Statistical inference techniques allow practitioners to construct confidence bounds around estimated metrics, while robust estimation mitigates the risk of spurious signals caused by small sample sizes or noisy predictions. NannyML provides implementations of CBPE and DLE, supporting metrics such as precision, recall, F1-score, and AUROC, all estimated without labels. This makes it possible to detect when a model is underperforming even before labels are collected, reducing blind spots in production monitoring.\n",
    "\n",
    "[Performance Estimation With Labels](https://www.nannyml.com/) refers to the direct evaluation of model predictions against actual ground truth outcomes once labels are available. Unlike label-free methods, this approach allows for precise calculation of traditional performance metrics such as accuracy, precision, recall, F1-score, AUROC, and calibration error. Monitoring with labels provides the most reliable indication of model performance, enabling fine-grained diagnosis of errors and biases. The advantage of having labels is the ability to attribute errors to specific subgroups, detect fairness violations, and conduct targeted retraining. Challenges include label delay, annotation quality, and ensuring that labels accurately reflect the operational environment. Common approaches include sliding window evaluation, where performance is tracked over recent data batches, and benchmark comparison, where production metrics are compared to baseline test set results. NannyML incorporates labeled performance tracking alongside its label-free estimators, allowing users to validate estimates once ground truth becomes available. This dual capability ensures consistency, improves confidence in label-free methods, and provides a comprehensive framework for performance monitoring in both short-term and long-term horizons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d4184",
   "metadata": {},
   "source": [
    "## 1.1. Data Background <a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "An open [Breast Cancer Dataset](https://www.kaggle.com/datasets/wasiqaliyasir/breast-cancer-dataset) from [Kaggle](https://www.kaggle.com/) (with all credits attributed to [Wasiq Ali](https://www.kaggle.com/wasiqaliyasir)) was used for the analysis as consolidated from the following primary sources: \n",
    "1. Reference Repository entitled **Differentiated breast Cancer Recurrence** from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/915/differentiated+breast+cancer+recurrence)\n",
    "2. Research Paper entitled **Nuclear Feature Extraction for Breast Tumor Diagnosis** from the [Electronic Imaging](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1905/1/Nuclear-feature-extraction-for-breast-tumor-diagnosis/10.1117/12.148698.short)\n",
    "\n",
    "This study hypothesized that the cell nuclei features derived from digitized images of fine needle aspirates (FNA) of breast masses influence breast cancer diagnoses between patients.\n",
    "\n",
    "The dichotomous categorical variable for the study is:\n",
    "* <span style=\"color: #FF0000\">diagnosis</span> - Status of the patient (M, Medical diagnosis of a cancerous breast tumor | B, Medical diagnosis of a non-cancerous breast tumor)\n",
    "\n",
    "The predictor variables for the study are:\n",
    "* <span style=\"color: #FF0000\">radius_mean</span> - Mean of the radius measurements (Mean of distances from center to points on the perimeter)\n",
    "* <span style=\"color: #FF0000\">texture_mean</span> - Mean of the texture measurements (Standard deviation of grayscale values)\n",
    "* <span style=\"color: #FF0000\">perimeter_mean</span> - Mean of the perimeter measurements\n",
    "* <span style=\"color: #FF0000\">area_mean</span> - Mean of the area measurements\n",
    "* <span style=\"color: #FF0000\">smoothness_mean</span> - Mean of the smoothness measurements (Local variation in radius lengths)\n",
    "* <span style=\"color: #FF0000\">compactness_mean</span> - Mean of the compactness measurements (Perimeter² / area - 1.0)\n",
    "* <span style=\"color: #FF0000\">concavity_mean</span> - Mean of the concavity measurements (Severity of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">concave points_mean</span> - Mean of the concave points measurements (Number of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">symmetry_mean</span> - Mean of the symmetry measurements\n",
    "* <span style=\"color: #FF0000\">fractal_dimension_mean</span> - Mean of the fractal dimension measurements (Coastline approximation - 1)\n",
    "* <span style=\"color: #FF0000\">radius_se</span> - Standard error of the radius measurements (Standard error of distances from center to points on the perimeter)\n",
    "* <span style=\"color: #FF0000\">texture_se</span> - Standard error of the texture measurements (Standard deviation of grayscale values)\n",
    "* <span style=\"color: #FF0000\">perimeter_se</span> - Standard error of the perimeter measurements\n",
    "* <span style=\"color: #FF0000\">area_se</span> - Standard error of the area measurements\n",
    "* <span style=\"color: #FF0000\">smoothness_se</span> - Standard error of the smoothness measurements (Local variation in radius lengths)\n",
    "* <span style=\"color: #FF0000\">compactness_se</span> - Standard error of the compactness measurements (Perimeter² / area - 1.0)\n",
    "* <span style=\"color: #FF0000\">concavity_se</span> - Standard error of the concavity measurements (Severity of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">concave points_se</span> - Standard error of the concave points measurements (Number of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">symmetry_se</span> - Standard error of the symmetry measurements\n",
    "* <span style=\"color: #FF0000\">fractal_dimension_se</span> - Standard error of the fractal dimension measurements (Coastline approximation - 1)\n",
    "* <span style=\"color: #FF0000\">radius_worst</span> - Largest value of the radius measurements (Largest value of distances from center to points on the perimeter)\n",
    "* <span style=\"color: #FF0000\">texture_worst</span> - Largest value of the texture measurements (Standard deviation of grayscale values)\n",
    "* <span style=\"color: #FF0000\">perimeter_worst</span> - Largest value of the perimeter measurements\n",
    "* <span style=\"color: #FF0000\">area_worst</span> - Largest value of the area measurements\n",
    "* <span style=\"color: #FF0000\">smoothness_worst</span> - Largest value of the smoothness measurements (Local variation in radius lengths)\n",
    "* <span style=\"color: #FF0000\">compactness_worst</span> - Largest value of the compactness measurements (Perimeter² / area - 1.0)\n",
    "* <span style=\"color: #FF0000\">concavity_worst</span> - Largest value of the concavity measurements (Severity of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">concave points_worst</span> - Largest value of the concave points measurements (Number of concave portions of the contour)\n",
    "* <span style=\"color: #FF0000\">symmetry_worst</span> - Largest value of the symmetry measurements\n",
    "* <span style=\"color: #FF0000\">fractal_dimension_worst</span> - Largest value of the fractal dimension measurements (Coastline approximation - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f7e47",
   "metadata": {},
   "source": [
    "## 1.2. Data Description <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "1. The initial tabular dataset was comprised of 569 observations and 32 variables (including 1 metadata, 1 target and 30 predictors).\n",
    "    * **569 rows** (observations)\n",
    "    * **32 columns** (variables)\n",
    "        * **1/32 metadata** (categorical)\n",
    "             * <span style=\"color: #FF0000\">id</span>\n",
    "        * **1/32 target** (categorical)\n",
    "             * <span style=\"color: #FF0000\">diagnosis</span>\n",
    "        * **30/32 predictor** (numeric)\n",
    "             * <span style=\"color: #FF0000\">radius_mean</span>\n",
    "             * <span style=\"color: #FF0000\">texture_mean</span>\n",
    "             * <span style=\"color: #FF0000\">perimeter_mean</span>\n",
    "             * <span style=\"color: #FF0000\">area_mean</span>\n",
    "             * <span style=\"color: #FF0000\">smoothness_mean</span>\n",
    "             * <span style=\"color: #FF0000\">compactness_mean</span>\n",
    "             * <span style=\"color: #FF0000\">concavity_mean</span>\n",
    "             * <span style=\"color: #FF0000\">concave points_mean</span>\n",
    "             * <span style=\"color: #FF0000\">symmetry_mean</span>\n",
    "             * <span style=\"color: #FF0000\">fractal_dimension_mean</span>\n",
    "             * <span style=\"color: #FF0000\">radius_se</span>\n",
    "             * <span style=\"color: #FF0000\">texture_se</span>\n",
    "             * <span style=\"color: #FF0000\">perimeter_se</span>\n",
    "             * <span style=\"color: #FF0000\">area_se</span>\n",
    "             * <span style=\"color: #FF0000\">smoothness_se</span>\n",
    "             * <span style=\"color: #FF0000\">compactness_se</span>\n",
    "             * <span style=\"color: #FF0000\">concavity_se</span>\n",
    "             * <span style=\"color: #FF0000\">concave points_se</span>\n",
    "             * <span style=\"color: #FF0000\">symmetry_se</span>\n",
    "             * <span style=\"color: #FF0000\">fractal_dimension_se</span>\n",
    "             * <span style=\"color: #FF0000\">radius_worst</span>\n",
    "             * <span style=\"color: #FF0000\">texture_worst</span>\n",
    "             * <span style=\"color: #FF0000\">perimeter_worst</span>\n",
    "             * <span style=\"color: #FF0000\">area_worst</span>\n",
    "             * <span style=\"color: #FF0000\">smoothness_worst</span>\n",
    "             * <span style=\"color: #FF0000\">compactness_worst</span>\n",
    "             * <span style=\"color: #FF0000\">concavity_worst</span>\n",
    "             * <span style=\"color: #FF0000\">concave points_worst</span>\n",
    "             * <span style=\"color: #FF0000\">symmetry_worst</span>\n",
    "             * <span style=\"color: #FF0000\">fractal_dimension_worst</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8cdc143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Loading Python Libraries\n",
    "##################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "\n",
    "from operator import truediv\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy import stats\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, StratifiedShuffleSplit\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "50c0cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Defining file paths\n",
    "##################################\n",
    "DATASETS_ORIGINAL_PATH = r\"datasets\\original\"\n",
    "DATASETS_FINAL_PATH = r\"datasets\\final\\complete\"\n",
    "DATASETS_FINAL_TRAIN_PATH = r\"datasets\\final\\train\"\n",
    "DATASETS_FINAL_TRAIN_FEATURES_PATH = r\"datasets\\final\\train\\features\"\n",
    "DATASETS_FINAL_TRAIN_TARGET_PATH = r\"datasets\\final\\train\\target\"\n",
    "DATASETS_FINAL_VALIDATION_PATH = r\"datasets\\final\\validation\"\n",
    "DATASETS_FINAL_VALIDATION_FEATURES_PATH = r\"datasets\\final\\validation\\features\"\n",
    "DATASETS_FINAL_VALIDATION_TARGET_PATH = r\"datasets\\final\\validation\\target\"\n",
    "DATASETS_FINAL_TEST_PATH = r\"datasets\\final\\test\"\n",
    "DATASETS_FINAL_TEST_FEATURES_PATH = r\"datasets\\final\\test\\features\"\n",
    "DATASETS_FINAL_TEST_TARGET_PATH = r\"datasets\\final\\test\\target\"\n",
    "DATASETS_PREPROCESSED_PATH = r\"datasets\\preprocessed\"\n",
    "DATASETS_PREPROCESSED_TRAIN_PATH = r\"datasets\\preprocessed\\train\"\n",
    "DATASETS_PREPROCESSED_TRAIN_FEATURES_PATH = r\"datasets\\preprocessed\\train\\features\"\n",
    "DATASETS_PREPROCESSED_TRAIN_TARGET_PATH = r\"datasets\\preprocessed\\train\\target\"\n",
    "DATASETS_PREPROCESSED_VALIDATION_PATH = r\"datasets\\preprocessed\\validation\"\n",
    "DATASETS_PREPROCESSED_VALIDATION_FEATURES_PATH = r\"datasets\\preprocessed\\validation\\features\"\n",
    "DATASETS_PREPROCESSED_VALIDATION_TARGET_PATH = r\"datasets\\preprocessed\\validation\\target\"\n",
    "DATASETS_PREPROCESSED_TEST_PATH = r\"datasets\\preprocessed\\test\"\n",
    "DATASETS_PREPROCESSED_TEST_FEATURES_PATH = r\"datasets\\preprocessed\\test\\features\"\n",
    "DATASETS_PREPROCESSED_TEST_TARGET_PATH = r\"datasets\\preprocessed\\test\\target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "830136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Loading the dataset\n",
    "# from the DATASETS_ORIGINAL_PATH\n",
    "##################################\n",
    "breast_cancer = pd.read_csv(os.path.join(\"..\", DATASETS_ORIGINAL_PATH, \"Breast_Cancer_Dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0740f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dimensions: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Performing a general exploration of the dataset\n",
    "##################################\n",
    "print('Dataset Dimensions: ')\n",
    "display(breast_cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cd84b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names and Data Types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                           int64\n",
       "diagnosis                   object\n",
       "radius_mean                float64\n",
       "texture_mean               float64\n",
       "perimeter_mean             float64\n",
       "area_mean                  float64\n",
       "smoothness_mean            float64\n",
       "compactness_mean           float64\n",
       "concavity_mean             float64\n",
       "concave points_mean        float64\n",
       "symmetry_mean              float64\n",
       "fractal_dimension_mean     float64\n",
       "radius_se                  float64\n",
       "texture_se                 float64\n",
       "perimeter_se               float64\n",
       "area_se                    float64\n",
       "smoothness_se              float64\n",
       "compactness_se             float64\n",
       "concavity_se               float64\n",
       "concave points_se          float64\n",
       "symmetry_se                float64\n",
       "fractal_dimension_se       float64\n",
       "radius_worst               float64\n",
       "texture_worst              float64\n",
       "perimeter_worst            float64\n",
       "area_worst                 float64\n",
       "smoothness_worst           float64\n",
       "compactness_worst          float64\n",
       "concavity_worst            float64\n",
       "concave points_worst       float64\n",
       "symmetry_worst             float64\n",
       "fractal_dimension_worst    float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Listing the column names and data types\n",
    "##################################\n",
    "print('Column Names and Data Types:')\n",
    "display(breast_cancer.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3c8f53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Setting the ID column as row names\n",
    "##################################\n",
    "breast_cancer = breast_cancer.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3cfc674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                         \n",
       "842302           M        17.99         10.38          122.80     1001.0   \n",
       "842517           M        20.57         17.77          132.90     1326.0   \n",
       "84300903         M        19.69         21.25          130.00     1203.0   \n",
       "84348301         M        11.42         20.38           77.58      386.1   \n",
       "84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                            \n",
       "842302            0.11840           0.27760          0.3001   \n",
       "842517            0.08474           0.07864          0.0869   \n",
       "84300903          0.10960           0.15990          0.1974   \n",
       "84348301          0.14250           0.28390          0.2414   \n",
       "84358402          0.10030           0.13280          0.1980   \n",
       "\n",
       "          concave points_mean  symmetry_mean  ...  radius_worst  \\\n",
       "id                                            ...                 \n",
       "842302                0.14710         0.2419  ...         25.38   \n",
       "842517                0.07017         0.1812  ...         24.99   \n",
       "84300903              0.12790         0.2069  ...         23.57   \n",
       "84348301              0.10520         0.2597  ...         14.91   \n",
       "84358402              0.10430         0.1809  ...         22.54   \n",
       "\n",
       "          texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "id                                                                       \n",
       "842302            17.33           184.60      2019.0            0.1622   \n",
       "842517            23.41           158.80      1956.0            0.1238   \n",
       "84300903          25.53           152.50      1709.0            0.1444   \n",
       "84348301          26.50            98.87       567.7            0.2098   \n",
       "84358402          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "          compactness_worst  concavity_worst  concave points_worst  \\\n",
       "id                                                                   \n",
       "842302               0.6656           0.7119                0.2654   \n",
       "842517               0.1866           0.2416                0.1860   \n",
       "84300903             0.4245           0.4504                0.2430   \n",
       "84348301             0.8663           0.6869                0.2575   \n",
       "84358402             0.2050           0.4000                0.1625   \n",
       "\n",
       "          symmetry_worst  fractal_dimension_worst  \n",
       "id                                                 \n",
       "842302            0.4601                  0.11890  \n",
       "842517            0.2750                  0.08902  \n",
       "84300903          0.3613                  0.08758  \n",
       "84348301          0.6638                  0.17300  \n",
       "84358402          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Taking a snapshot of the dataset\n",
    "##################################\n",
    "breast_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fb563785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Variable Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "radius_mean              569.0   14.127292    3.524049    6.981000   \n",
       "texture_mean             569.0   19.289649    4.301036    9.710000   \n",
       "perimeter_mean           569.0   91.969033   24.298981   43.790000   \n",
       "area_mean                569.0  654.889104  351.914129  143.500000   \n",
       "smoothness_mean          569.0    0.096360    0.014064    0.052630   \n",
       "compactness_mean         569.0    0.104341    0.052813    0.019380   \n",
       "concavity_mean           569.0    0.088799    0.079720    0.000000   \n",
       "concave points_mean      569.0    0.048919    0.038803    0.000000   \n",
       "symmetry_mean            569.0    0.181162    0.027414    0.106000   \n",
       "fractal_dimension_mean   569.0    0.062798    0.007060    0.049960   \n",
       "radius_se                569.0    0.405172    0.277313    0.111500   \n",
       "texture_se               569.0    1.216853    0.551648    0.360200   \n",
       "perimeter_se             569.0    2.866059    2.021855    0.757000   \n",
       "area_se                  569.0   40.337079   45.491006    6.802000   \n",
       "smoothness_se            569.0    0.007041    0.003003    0.001713   \n",
       "compactness_se           569.0    0.025478    0.017908    0.002252   \n",
       "concavity_se             569.0    0.031894    0.030186    0.000000   \n",
       "concave points_se        569.0    0.011796    0.006170    0.000000   \n",
       "symmetry_se              569.0    0.020542    0.008266    0.007882   \n",
       "fractal_dimension_se     569.0    0.003795    0.002646    0.000895   \n",
       "radius_worst             569.0   16.269190    4.833242    7.930000   \n",
       "texture_worst            569.0   25.677223    6.146258   12.020000   \n",
       "perimeter_worst          569.0  107.261213   33.602542   50.410000   \n",
       "area_worst               569.0  880.583128  569.356993  185.200000   \n",
       "smoothness_worst         569.0    0.132369    0.022832    0.071170   \n",
       "compactness_worst        569.0    0.254265    0.157336    0.027290   \n",
       "concavity_worst          569.0    0.272188    0.208624    0.000000   \n",
       "concave points_worst     569.0    0.114606    0.065732    0.000000   \n",
       "symmetry_worst           569.0    0.290076    0.061867    0.156500   \n",
       "fractal_dimension_worst  569.0    0.083946    0.018061    0.055040   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "radius_mean               11.700000   13.370000    15.780000    28.11000  \n",
       "texture_mean              16.170000   18.840000    21.800000    39.28000  \n",
       "perimeter_mean            75.170000   86.240000   104.100000   188.50000  \n",
       "area_mean                420.300000  551.100000   782.700000  2501.00000  \n",
       "smoothness_mean            0.086370    0.095870     0.105300     0.16340  \n",
       "compactness_mean           0.064920    0.092630     0.130400     0.34540  \n",
       "concavity_mean             0.029560    0.061540     0.130700     0.42680  \n",
       "concave points_mean        0.020310    0.033500     0.074000     0.20120  \n",
       "symmetry_mean              0.161900    0.179200     0.195700     0.30400  \n",
       "fractal_dimension_mean     0.057700    0.061540     0.066120     0.09744  \n",
       "radius_se                  0.232400    0.324200     0.478900     2.87300  \n",
       "texture_se                 0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter_se               1.606000    2.287000     3.357000    21.98000  \n",
       "area_se                   17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness_se              0.005169    0.006380     0.008146     0.03113  \n",
       "compactness_se             0.013080    0.020450     0.032450     0.13540  \n",
       "concavity_se               0.015090    0.025890     0.042050     0.39600  \n",
       "concave points_se          0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry_se                0.015160    0.018730     0.023480     0.07895  \n",
       "fractal_dimension_se       0.002248    0.003187     0.004558     0.02984  \n",
       "radius_worst              13.010000   14.970000    18.790000    36.04000  \n",
       "texture_worst             21.080000   25.410000    29.720000    49.54000  \n",
       "perimeter_worst           84.110000   97.660000   125.400000   251.20000  \n",
       "area_worst               515.300000  686.500000  1084.000000  4254.00000  \n",
       "smoothness_worst           0.116600    0.131300     0.146000     0.22260  \n",
       "compactness_worst          0.147200    0.211900     0.339100     1.05800  \n",
       "concavity_worst            0.114500    0.226700     0.382900     1.25200  \n",
       "concave points_worst       0.064930    0.099930     0.161400     0.29100  \n",
       "symmetry_worst             0.250400    0.282200     0.317900     0.66380  \n",
       "fractal_dimension_worst    0.071460    0.080040     0.092080     0.20750  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Performing a general exploration of the numeric variables\n",
    "##################################\n",
    "print('Numeric Variable Summary:')\n",
    "display(breast_cancer.describe(include='number').transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5616b",
   "metadata": {},
   "source": [
    "## 1.3. Data Quality Assessment <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "Data quality findings based on assessment are as follows:\n",
    "1. No duplicated rows were noted.\n",
    "2. No missing data noted for any variable with Null.Count>0 and Fill.Rate<1.0.\n",
    "3. No low variance observed for any variable with First.Second.Mode.Ratio>5.\n",
    "4. No low variance observed for any variable with Unique.Count.Ratio>10.\n",
    "5. High skewness observed for 5 variables with Skewness>3 or Skewness<(-3).\n",
    "    * <span style=\"color: #FF0000\">area_se</span>: Skewness = 5.447\n",
    "    * <span style=\"color: #FF0000\">concavity_se</span>: Skewness = 5.110\n",
    "    * <span style=\"color: #FF0000\">fractal_dimension_se</span>: Skewness = 3.923\n",
    "    * <span style=\"color: #FF0000\">perimeter_se</span>: Skewness = 3.443\n",
    "    * <span style=\"color: #FF0000\">radius_se</span>: Skewness = 3.088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9d32c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of duplicated rows\n",
    "##################################\n",
    "breast_cancer.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "57ec602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the data types for each column\n",
    "##################################\n",
    "data_type_list = list(breast_cancer.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db2deea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the variable names for each column\n",
    "##################################\n",
    "variable_name_list = list(breast_cancer.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f9c78662",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of observations for each column\n",
    "##################################\n",
    "row_count_list = list([len(breast_cancer)] * len(breast_cancer.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "245a8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of missing data for each column\n",
    "##################################\n",
    "null_count_list = list(breast_cancer.isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "49e18a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of non-missing data for each column\n",
    "##################################\n",
    "non_null_count_list = list(breast_cancer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7b31c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the missing data percentage for each column\n",
    "##################################\n",
    "fill_rate_list = map(truediv, non_null_count_list, row_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a14c673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column.Name</th>\n",
       "      <th>Column.Type</th>\n",
       "      <th>Row.Count</th>\n",
       "      <th>Non.Null.Count</th>\n",
       "      <th>Null.Count</th>\n",
       "      <th>Fill.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diagnosis</td>\n",
       "      <td>object</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>radius_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texture_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perimeter_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smoothness_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>compactness_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>concavity_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>concave points_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>symmetry_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>radius_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>texture_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>perimeter_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>area_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smoothness_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compactness_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>concavity_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>concave points_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>symmetry_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>radius_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>texture_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>perimeter_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>area_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>smoothness_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>compactness_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>concavity_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>concave points_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>symmetry_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "      <td>float64</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Column.Name Column.Type  Row.Count  Non.Null.Count  \\\n",
       "0                 diagnosis      object        569             569   \n",
       "1               radius_mean     float64        569             569   \n",
       "2              texture_mean     float64        569             569   \n",
       "3            perimeter_mean     float64        569             569   \n",
       "4                 area_mean     float64        569             569   \n",
       "5           smoothness_mean     float64        569             569   \n",
       "6          compactness_mean     float64        569             569   \n",
       "7            concavity_mean     float64        569             569   \n",
       "8       concave points_mean     float64        569             569   \n",
       "9             symmetry_mean     float64        569             569   \n",
       "10   fractal_dimension_mean     float64        569             569   \n",
       "11                radius_se     float64        569             569   \n",
       "12               texture_se     float64        569             569   \n",
       "13             perimeter_se     float64        569             569   \n",
       "14                  area_se     float64        569             569   \n",
       "15            smoothness_se     float64        569             569   \n",
       "16           compactness_se     float64        569             569   \n",
       "17             concavity_se     float64        569             569   \n",
       "18        concave points_se     float64        569             569   \n",
       "19              symmetry_se     float64        569             569   \n",
       "20     fractal_dimension_se     float64        569             569   \n",
       "21             radius_worst     float64        569             569   \n",
       "22            texture_worst     float64        569             569   \n",
       "23          perimeter_worst     float64        569             569   \n",
       "24               area_worst     float64        569             569   \n",
       "25         smoothness_worst     float64        569             569   \n",
       "26        compactness_worst     float64        569             569   \n",
       "27          concavity_worst     float64        569             569   \n",
       "28     concave points_worst     float64        569             569   \n",
       "29           symmetry_worst     float64        569             569   \n",
       "30  fractal_dimension_worst     float64        569             569   \n",
       "\n",
       "    Null.Count  Fill.Rate  \n",
       "0            0        1.0  \n",
       "1            0        1.0  \n",
       "2            0        1.0  \n",
       "3            0        1.0  \n",
       "4            0        1.0  \n",
       "5            0        1.0  \n",
       "6            0        1.0  \n",
       "7            0        1.0  \n",
       "8            0        1.0  \n",
       "9            0        1.0  \n",
       "10           0        1.0  \n",
       "11           0        1.0  \n",
       "12           0        1.0  \n",
       "13           0        1.0  \n",
       "14           0        1.0  \n",
       "15           0        1.0  \n",
       "16           0        1.0  \n",
       "17           0        1.0  \n",
       "18           0        1.0  \n",
       "19           0        1.0  \n",
       "20           0        1.0  \n",
       "21           0        1.0  \n",
       "22           0        1.0  \n",
       "23           0        1.0  \n",
       "24           0        1.0  \n",
       "25           0        1.0  \n",
       "26           0        1.0  \n",
       "27           0        1.0  \n",
       "28           0        1.0  \n",
       "29           0        1.0  \n",
       "30           0        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Formulating the summary\n",
    "# for all columns\n",
    "##################################\n",
    "all_column_quality_summary = pd.DataFrame(zip(variable_name_list,\n",
    "                                              data_type_list,\n",
    "                                              row_count_list,\n",
    "                                              non_null_count_list,\n",
    "                                              null_count_list,\n",
    "                                              fill_rate_list), \n",
    "                                        columns=['Column.Name',\n",
    "                                                 'Column.Type',\n",
    "                                                 'Row.Count',\n",
    "                                                 'Non.Null.Count',\n",
    "                                                 'Null.Count',                                                 \n",
    "                                                 'Fill.Rate'])\n",
    "display(all_column_quality_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "476d7089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of columns\n",
    "# with Fill.Rate < 1.00\n",
    "##################################\n",
    "len(all_column_quality_summary[(all_column_quality_summary['Fill.Rate']<1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01cfc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Identifying the rows\n",
    "# with Fill.Rate < 0.90\n",
    "##################################\n",
    "column_low_fill_rate = all_column_quality_summary[(all_column_quality_summary['Fill.Rate']<0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1851f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the indices for each observation\n",
    "##################################\n",
    "row_index_list = breast_cancer.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f35e0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of columns for each observation\n",
    "##################################\n",
    "column_count_list = list([len(breast_cancer.columns)] * len(breast_cancer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc71d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of missing data for each row\n",
    "##################################\n",
    "null_row_list = list(breast_cancer.isna().sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "63e34ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the missing data percentage for each column\n",
    "##################################\n",
    "missing_rate_list = map(truediv, null_row_list, column_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cd34457a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row.Name</th>\n",
       "      <th>Column.Count</th>\n",
       "      <th>Null.Count</th>\n",
       "      <th>Missing.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Row.Name  Column.Count  Null.Count  Missing.Rate\n",
       "0      842302            31           0           0.0\n",
       "1      842517            31           0           0.0\n",
       "2    84300903            31           0           0.0\n",
       "3    84348301            31           0           0.0\n",
       "4    84358402            31           0           0.0\n",
       "..        ...           ...         ...           ...\n",
       "564    926424            31           0           0.0\n",
       "565    926682            31           0           0.0\n",
       "566    926954            31           0           0.0\n",
       "567    927241            31           0           0.0\n",
       "568     92751            31           0           0.0\n",
       "\n",
       "[569 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Identifying the rows\n",
    "# with missing data\n",
    "##################################\n",
    "all_row_quality_summary = pd.DataFrame(zip(row_index_list,\n",
    "                                           column_count_list,\n",
    "                                           null_row_list,\n",
    "                                           missing_rate_list), \n",
    "                                        columns=['Row.Name',\n",
    "                                                 'Column.Count',\n",
    "                                                 'Null.Count',                                                 \n",
    "                                                 'Missing.Rate'])\n",
    "display(all_row_quality_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "549a1497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of rows\n",
    "# with Missing.Rate > 0.00\n",
    "##################################\n",
    "len(all_row_quality_summary[(all_row_quality_summary['Missing.Rate']>0.00)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "17b8d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Formulating the dataset\n",
    "# with numeric columns only\n",
    "##################################\n",
    "thyroid_cancer_numeric = breast_cancer.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b76fea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the variable names for each numeric column\n",
    "##################################\n",
    "numeric_variable_name_list = thyroid_cancer_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e85cbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the minimum value for each numeric column\n",
    "##################################\n",
    "numeric_minimum_list = thyroid_cancer_numeric.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a286a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the mean value for each numeric column\n",
    "##################################\n",
    "numeric_mean_list = thyroid_cancer_numeric.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "07ebaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the median value for each numeric column\n",
    "##################################\n",
    "numeric_median_list = thyroid_cancer_numeric.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a89c51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the maximum value for each numeric column\n",
    "##################################\n",
    "numeric_maximum_list = thyroid_cancer_numeric.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d2aa280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the first mode values for each numeric column\n",
    "##################################\n",
    "numeric_first_mode_list = [breast_cancer[x].value_counts(dropna=True).index.tolist()[0] for x in thyroid_cancer_numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "13229883",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the second mode values for each numeric column\n",
    "##################################\n",
    "numeric_second_mode_list = [breast_cancer[x].value_counts(dropna=True).index.tolist()[1] for x in thyroid_cancer_numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "19b0da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of first mode values for each numeric column\n",
    "##################################\n",
    "numeric_first_mode_count_list = [thyroid_cancer_numeric[x].isin([breast_cancer[x].value_counts(dropna=True).index.tolist()[0]]).sum() for x in thyroid_cancer_numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8fa0b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of second mode values for each numeric column\n",
    "##################################\n",
    "numeric_second_mode_count_list = [thyroid_cancer_numeric[x].isin([breast_cancer[x].value_counts(dropna=True).index.tolist()[1]]).sum() for x in thyroid_cancer_numeric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9b4c100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the first mode to second mode ratio for each numeric column\n",
    "##################################\n",
    "numeric_first_second_mode_ratio_list = map(truediv, numeric_first_mode_count_list, numeric_second_mode_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9dd7da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of unique values for each numeric column\n",
    "##################################\n",
    "numeric_unique_count_list = thyroid_cancer_numeric.nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6b25777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of observations for each numeric column\n",
    "##################################\n",
    "numeric_row_count_list = list([len(thyroid_cancer_numeric)] * len(thyroid_cancer_numeric.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7b1ff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the unique to count ratio for each numeric column\n",
    "##################################\n",
    "numeric_unique_count_ratio_list = map(truediv, numeric_unique_count_list, numeric_row_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "32d6bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the skewness value for each numeric column\n",
    "##################################\n",
    "numeric_skewness_list = thyroid_cancer_numeric.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8cf1c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the kurtosis value for each numeric column\n",
    "##################################\n",
    "numeric_kurtosis_list = thyroid_cancer_numeric.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "00cf66b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numeric.Column.Name</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>First.Mode</th>\n",
       "      <th>Second.Mode</th>\n",
       "      <th>First.Mode.Count</th>\n",
       "      <th>Second.Mode.Count</th>\n",
       "      <th>First.Second.Mode.Ratio</th>\n",
       "      <th>Unique.Count</th>\n",
       "      <th>Row.Count</th>\n",
       "      <th>Unique.Count.Ratio</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>radius_mean</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>28.11000</td>\n",
       "      <td>12.340000</td>\n",
       "      <td>11.060000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>456</td>\n",
       "      <td>569</td>\n",
       "      <td>0.801406</td>\n",
       "      <td>0.942380</td>\n",
       "      <td>0.845522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>texture_mean</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>39.28000</td>\n",
       "      <td>16.840000</td>\n",
       "      <td>19.830000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>479</td>\n",
       "      <td>569</td>\n",
       "      <td>0.841828</td>\n",
       "      <td>0.650450</td>\n",
       "      <td>0.758319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perimeter_mean</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>188.50000</td>\n",
       "      <td>82.610000</td>\n",
       "      <td>134.700000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>522</td>\n",
       "      <td>569</td>\n",
       "      <td>0.917399</td>\n",
       "      <td>0.990650</td>\n",
       "      <td>0.972214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>area_mean</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>2501.00000</td>\n",
       "      <td>512.200000</td>\n",
       "      <td>394.100000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>539</td>\n",
       "      <td>569</td>\n",
       "      <td>0.947276</td>\n",
       "      <td>1.645732</td>\n",
       "      <td>3.652303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smoothness_mean</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.16340</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>474</td>\n",
       "      <td>569</td>\n",
       "      <td>0.833040</td>\n",
       "      <td>0.456324</td>\n",
       "      <td>0.855975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>compactness_mean</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.34540</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>537</td>\n",
       "      <td>569</td>\n",
       "      <td>0.943761</td>\n",
       "      <td>1.190123</td>\n",
       "      <td>1.650130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>concavity_mean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.42680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>537</td>\n",
       "      <td>569</td>\n",
       "      <td>0.943761</td>\n",
       "      <td>1.401180</td>\n",
       "      <td>1.998638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>concave points_mean</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.20120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028640</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>542</td>\n",
       "      <td>569</td>\n",
       "      <td>0.952548</td>\n",
       "      <td>1.171180</td>\n",
       "      <td>1.066556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>symmetry_mean</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.30400</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>432</td>\n",
       "      <td>569</td>\n",
       "      <td>0.759227</td>\n",
       "      <td>0.725609</td>\n",
       "      <td>1.287933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fractal_dimension_mean</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.067820</td>\n",
       "      <td>0.061130</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>499</td>\n",
       "      <td>569</td>\n",
       "      <td>0.876977</td>\n",
       "      <td>1.304489</td>\n",
       "      <td>3.005892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>radius_se</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>2.87300</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>540</td>\n",
       "      <td>569</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>3.088612</td>\n",
       "      <td>17.686726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>texture_se</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>4.88500</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>519</td>\n",
       "      <td>569</td>\n",
       "      <td>0.912127</td>\n",
       "      <td>1.646444</td>\n",
       "      <td>5.349169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>perimeter_se</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>21.98000</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.143000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>533</td>\n",
       "      <td>569</td>\n",
       "      <td>0.936731</td>\n",
       "      <td>3.443615</td>\n",
       "      <td>21.401905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>area_se</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>542.20000</td>\n",
       "      <td>16.970000</td>\n",
       "      <td>16.640000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>528</td>\n",
       "      <td>569</td>\n",
       "      <td>0.927944</td>\n",
       "      <td>5.447186</td>\n",
       "      <td>49.209077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoothness_se</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.03113</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>547</td>\n",
       "      <td>569</td>\n",
       "      <td>0.961336</td>\n",
       "      <td>2.314450</td>\n",
       "      <td>10.469840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>compactness_se</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.13540</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>541</td>\n",
       "      <td>569</td>\n",
       "      <td>0.950791</td>\n",
       "      <td>1.902221</td>\n",
       "      <td>5.106252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>concavity_se</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.39600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>533</td>\n",
       "      <td>569</td>\n",
       "      <td>0.936731</td>\n",
       "      <td>5.110463</td>\n",
       "      <td>48.861395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>concave points_se</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.05279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>507</td>\n",
       "      <td>569</td>\n",
       "      <td>0.891037</td>\n",
       "      <td>1.444678</td>\n",
       "      <td>5.126302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>symmetry_se</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.07895</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>498</td>\n",
       "      <td>569</td>\n",
       "      <td>0.875220</td>\n",
       "      <td>2.195133</td>\n",
       "      <td>7.896130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.02984</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>545</td>\n",
       "      <td>569</td>\n",
       "      <td>0.957821</td>\n",
       "      <td>3.923969</td>\n",
       "      <td>26.280847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>radius_worst</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>36.04000</td>\n",
       "      <td>12.360000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>457</td>\n",
       "      <td>569</td>\n",
       "      <td>0.803163</td>\n",
       "      <td>1.103115</td>\n",
       "      <td>0.944090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>texture_worst</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>49.54000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>27.260000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>511</td>\n",
       "      <td>569</td>\n",
       "      <td>0.898067</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>0.224302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>perimeter_worst</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>251.20000</td>\n",
       "      <td>117.700000</td>\n",
       "      <td>105.900000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>569</td>\n",
       "      <td>0.903339</td>\n",
       "      <td>1.128164</td>\n",
       "      <td>1.070150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>area_worst</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>4254.00000</td>\n",
       "      <td>698.800000</td>\n",
       "      <td>808.900000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>544</td>\n",
       "      <td>569</td>\n",
       "      <td>0.956063</td>\n",
       "      <td>1.859373</td>\n",
       "      <td>4.396395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>smoothness_worst</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.22260</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>411</td>\n",
       "      <td>569</td>\n",
       "      <td>0.722320</td>\n",
       "      <td>0.415426</td>\n",
       "      <td>0.517825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>compactness_worst</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>529</td>\n",
       "      <td>569</td>\n",
       "      <td>0.929701</td>\n",
       "      <td>1.473555</td>\n",
       "      <td>3.039288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>concavity_worst</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>539</td>\n",
       "      <td>569</td>\n",
       "      <td>0.947276</td>\n",
       "      <td>1.150237</td>\n",
       "      <td>1.615253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>concave points_worst</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.29100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>492</td>\n",
       "      <td>569</td>\n",
       "      <td>0.864675</td>\n",
       "      <td>0.492616</td>\n",
       "      <td>-0.535535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>symmetry_worst</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.66380</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>500</td>\n",
       "      <td>569</td>\n",
       "      <td>0.878735</td>\n",
       "      <td>1.433928</td>\n",
       "      <td>4.444560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fractal_dimension_worst</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0.074270</td>\n",
       "      <td>0.087010</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>535</td>\n",
       "      <td>569</td>\n",
       "      <td>0.940246</td>\n",
       "      <td>1.662579</td>\n",
       "      <td>5.244611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Numeric.Column.Name     Minimum        Mean      Median     Maximum  \\\n",
       "0               radius_mean    6.981000   14.127292   13.370000    28.11000   \n",
       "1              texture_mean    9.710000   19.289649   18.840000    39.28000   \n",
       "2            perimeter_mean   43.790000   91.969033   86.240000   188.50000   \n",
       "3                 area_mean  143.500000  654.889104  551.100000  2501.00000   \n",
       "4           smoothness_mean    0.052630    0.096360    0.095870     0.16340   \n",
       "5          compactness_mean    0.019380    0.104341    0.092630     0.34540   \n",
       "6            concavity_mean    0.000000    0.088799    0.061540     0.42680   \n",
       "7       concave points_mean    0.000000    0.048919    0.033500     0.20120   \n",
       "8             symmetry_mean    0.106000    0.181162    0.179200     0.30400   \n",
       "9    fractal_dimension_mean    0.049960    0.062798    0.061540     0.09744   \n",
       "10                radius_se    0.111500    0.405172    0.324200     2.87300   \n",
       "11               texture_se    0.360200    1.216853    1.108000     4.88500   \n",
       "12             perimeter_se    0.757000    2.866059    2.287000    21.98000   \n",
       "13                  area_se    6.802000   40.337079   24.530000   542.20000   \n",
       "14            smoothness_se    0.001713    0.007041    0.006380     0.03113   \n",
       "15           compactness_se    0.002252    0.025478    0.020450     0.13540   \n",
       "16             concavity_se    0.000000    0.031894    0.025890     0.39600   \n",
       "17        concave points_se    0.000000    0.011796    0.010930     0.05279   \n",
       "18              symmetry_se    0.007882    0.020542    0.018730     0.07895   \n",
       "19     fractal_dimension_se    0.000895    0.003795    0.003187     0.02984   \n",
       "20             radius_worst    7.930000   16.269190   14.970000    36.04000   \n",
       "21            texture_worst   12.020000   25.677223   25.410000    49.54000   \n",
       "22          perimeter_worst   50.410000  107.261213   97.660000   251.20000   \n",
       "23               area_worst  185.200000  880.583128  686.500000  4254.00000   \n",
       "24         smoothness_worst    0.071170    0.132369    0.131300     0.22260   \n",
       "25        compactness_worst    0.027290    0.254265    0.211900     1.05800   \n",
       "26          concavity_worst    0.000000    0.272188    0.226700     1.25200   \n",
       "27     concave points_worst    0.000000    0.114606    0.099930     0.29100   \n",
       "28           symmetry_worst    0.156500    0.290076    0.282200     0.66380   \n",
       "29  fractal_dimension_worst    0.055040    0.083946    0.080040     0.20750   \n",
       "\n",
       "    First.Mode  Second.Mode  First.Mode.Count  Second.Mode.Count  \\\n",
       "0    12.340000    11.060000                 4                  3   \n",
       "1    16.840000    19.830000                 3                  3   \n",
       "2    82.610000   134.700000                 3                  3   \n",
       "3   512.200000   394.100000                 3                  2   \n",
       "4     0.100700     0.105400                 5                  4   \n",
       "5     0.114700     0.120600                 3                  3   \n",
       "6     0.000000     0.120400                13                  3   \n",
       "7     0.000000     0.028640                13                  3   \n",
       "8     0.176900     0.189300                 4                  4   \n",
       "9     0.067820     0.061130                 3                  3   \n",
       "10    0.286000     0.220400                 3                  3   \n",
       "11    0.856100     1.350000                 3                  3   \n",
       "12    1.778000     1.143000                 4                  2   \n",
       "13   16.970000    16.640000                 3                  3   \n",
       "14    0.005910     0.006064                 2                  2   \n",
       "15    0.018120     0.011040                 3                  3   \n",
       "16    0.000000     0.021850                13                  2   \n",
       "17    0.000000     0.011670                13                  3   \n",
       "18    0.013440     0.020450                 4                  3   \n",
       "19    0.002256     0.002205                 2                  2   \n",
       "20   12.360000    13.500000                 5                  4   \n",
       "21   17.700000    27.260000                 3                  3   \n",
       "22  117.700000   105.900000                 3                  3   \n",
       "23  698.800000   808.900000                 2                  2   \n",
       "24    0.140100     0.131200                 4                  4   \n",
       "25    0.148600     0.341600                 3                  3   \n",
       "26    0.000000     0.450400                13                  3   \n",
       "27    0.000000     0.110500                13                  3   \n",
       "28    0.236900     0.310900                 3                  3   \n",
       "29    0.074270     0.087010                 3                  2   \n",
       "\n",
       "    First.Second.Mode.Ratio  Unique.Count  Row.Count  Unique.Count.Ratio  \\\n",
       "0                  1.333333           456        569            0.801406   \n",
       "1                  1.000000           479        569            0.841828   \n",
       "2                  1.000000           522        569            0.917399   \n",
       "3                  1.500000           539        569            0.947276   \n",
       "4                  1.250000           474        569            0.833040   \n",
       "5                  1.000000           537        569            0.943761   \n",
       "6                  4.333333           537        569            0.943761   \n",
       "7                  4.333333           542        569            0.952548   \n",
       "8                  1.000000           432        569            0.759227   \n",
       "9                  1.000000           499        569            0.876977   \n",
       "10                 1.000000           540        569            0.949033   \n",
       "11                 1.000000           519        569            0.912127   \n",
       "12                 2.000000           533        569            0.936731   \n",
       "13                 1.000000           528        569            0.927944   \n",
       "14                 1.000000           547        569            0.961336   \n",
       "15                 1.000000           541        569            0.950791   \n",
       "16                 6.500000           533        569            0.936731   \n",
       "17                 4.333333           507        569            0.891037   \n",
       "18                 1.333333           498        569            0.875220   \n",
       "19                 1.000000           545        569            0.957821   \n",
       "20                 1.250000           457        569            0.803163   \n",
       "21                 1.000000           511        569            0.898067   \n",
       "22                 1.000000           514        569            0.903339   \n",
       "23                 1.000000           544        569            0.956063   \n",
       "24                 1.000000           411        569            0.722320   \n",
       "25                 1.000000           529        569            0.929701   \n",
       "26                 4.333333           539        569            0.947276   \n",
       "27                 4.333333           492        569            0.864675   \n",
       "28                 1.000000           500        569            0.878735   \n",
       "29                 1.500000           535        569            0.940246   \n",
       "\n",
       "    Skewness   Kurtosis  \n",
       "0   0.942380   0.845522  \n",
       "1   0.650450   0.758319  \n",
       "2   0.990650   0.972214  \n",
       "3   1.645732   3.652303  \n",
       "4   0.456324   0.855975  \n",
       "5   1.190123   1.650130  \n",
       "6   1.401180   1.998638  \n",
       "7   1.171180   1.066556  \n",
       "8   0.725609   1.287933  \n",
       "9   1.304489   3.005892  \n",
       "10  3.088612  17.686726  \n",
       "11  1.646444   5.349169  \n",
       "12  3.443615  21.401905  \n",
       "13  5.447186  49.209077  \n",
       "14  2.314450  10.469840  \n",
       "15  1.902221   5.106252  \n",
       "16  5.110463  48.861395  \n",
       "17  1.444678   5.126302  \n",
       "18  2.195133   7.896130  \n",
       "19  3.923969  26.280847  \n",
       "20  1.103115   0.944090  \n",
       "21  0.498321   0.224302  \n",
       "22  1.128164   1.070150  \n",
       "23  1.859373   4.396395  \n",
       "24  0.415426   0.517825  \n",
       "25  1.473555   3.039288  \n",
       "26  1.150237   1.615253  \n",
       "27  0.492616  -0.535535  \n",
       "28  1.433928   4.444560  \n",
       "29  1.662579   5.244611  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Generating a column quality summary for the numeric column\n",
    "##################################\n",
    "numeric_column_quality_summary = pd.DataFrame(zip(numeric_variable_name_list,\n",
    "                                                numeric_minimum_list,\n",
    "                                                numeric_mean_list,\n",
    "                                                numeric_median_list,\n",
    "                                                numeric_maximum_list,\n",
    "                                                numeric_first_mode_list,\n",
    "                                                numeric_second_mode_list,\n",
    "                                                numeric_first_mode_count_list,\n",
    "                                                numeric_second_mode_count_list,\n",
    "                                                numeric_first_second_mode_ratio_list,\n",
    "                                                numeric_unique_count_list,\n",
    "                                                numeric_row_count_list,\n",
    "                                                numeric_unique_count_ratio_list,\n",
    "                                                numeric_skewness_list,\n",
    "                                                numeric_kurtosis_list), \n",
    "                                        columns=['Numeric.Column.Name',\n",
    "                                                 'Minimum',\n",
    "                                                 'Mean',\n",
    "                                                 'Median',\n",
    "                                                 'Maximum',\n",
    "                                                 'First.Mode',\n",
    "                                                 'Second.Mode',\n",
    "                                                 'First.Mode.Count',\n",
    "                                                 'Second.Mode.Count',\n",
    "                                                 'First.Second.Mode.Ratio',\n",
    "                                                 'Unique.Count',\n",
    "                                                 'Row.Count',\n",
    "                                                 'Unique.Count.Ratio',\n",
    "                                                 'Skewness',\n",
    "                                                 'Kurtosis'])\n",
    "display(numeric_column_quality_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5d93466c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of numeric columns\n",
    "# with First.Second.Mode.Ratio > 5.00\n",
    "##################################\n",
    "len(numeric_column_quality_summary[(numeric_column_quality_summary['First.Second.Mode.Ratio']>10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d76f5daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of numeric columns\n",
    "# with Unique.Count.Ratio > 10.00\n",
    "##################################\n",
    "len(numeric_column_quality_summary[(numeric_column_quality_summary['Unique.Count.Ratio']>10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a5732d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################\n",
    "# Counting the number of numeric columns\n",
    "# with Skewness > 3.00 or Skewness < -3.00\n",
    "##################################\n",
    "len(numeric_column_quality_summary[(numeric_column_quality_summary['Skewness']>3) | (numeric_column_quality_summary['Skewness']<(-3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c5c29fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numeric.Column.Name</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>First.Mode</th>\n",
       "      <th>Second.Mode</th>\n",
       "      <th>First.Mode.Count</th>\n",
       "      <th>Second.Mode.Count</th>\n",
       "      <th>First.Second.Mode.Ratio</th>\n",
       "      <th>Unique.Count</th>\n",
       "      <th>Row.Count</th>\n",
       "      <th>Unique.Count.Ratio</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>area_se</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>542.20000</td>\n",
       "      <td>16.970000</td>\n",
       "      <td>16.640000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>528</td>\n",
       "      <td>569</td>\n",
       "      <td>0.927944</td>\n",
       "      <td>5.447186</td>\n",
       "      <td>49.209077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>concavity_se</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.39600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>533</td>\n",
       "      <td>569</td>\n",
       "      <td>0.936731</td>\n",
       "      <td>5.110463</td>\n",
       "      <td>48.861395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fractal_dimension_se</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.02984</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>545</td>\n",
       "      <td>569</td>\n",
       "      <td>0.957821</td>\n",
       "      <td>3.923969</td>\n",
       "      <td>26.280847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>perimeter_se</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>21.98000</td>\n",
       "      <td>1.778000</td>\n",
       "      <td>1.143000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>533</td>\n",
       "      <td>569</td>\n",
       "      <td>0.936731</td>\n",
       "      <td>3.443615</td>\n",
       "      <td>21.401905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>radius_se</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>2.87300</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>540</td>\n",
       "      <td>569</td>\n",
       "      <td>0.949033</td>\n",
       "      <td>3.088612</td>\n",
       "      <td>17.686726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Numeric.Column.Name   Minimum       Mean     Median    Maximum  \\\n",
       "13               area_se  6.802000  40.337079  24.530000  542.20000   \n",
       "16          concavity_se  0.000000   0.031894   0.025890    0.39600   \n",
       "19  fractal_dimension_se  0.000895   0.003795   0.003187    0.02984   \n",
       "12          perimeter_se  0.757000   2.866059   2.287000   21.98000   \n",
       "10             radius_se  0.111500   0.405172   0.324200    2.87300   \n",
       "\n",
       "    First.Mode  Second.Mode  First.Mode.Count  Second.Mode.Count  \\\n",
       "13   16.970000    16.640000                 3                  3   \n",
       "16    0.000000     0.021850                13                  2   \n",
       "19    0.002256     0.002205                 2                  2   \n",
       "12    1.778000     1.143000                 4                  2   \n",
       "10    0.286000     0.220400                 3                  3   \n",
       "\n",
       "    First.Second.Mode.Ratio  Unique.Count  Row.Count  Unique.Count.Ratio  \\\n",
       "13                      1.0           528        569            0.927944   \n",
       "16                      6.5           533        569            0.936731   \n",
       "19                      1.0           545        569            0.957821   \n",
       "12                      2.0           533        569            0.936731   \n",
       "10                      1.0           540        569            0.949033   \n",
       "\n",
       "    Skewness   Kurtosis  \n",
       "13  5.447186  49.209077  \n",
       "16  5.110463  48.861395  \n",
       "19  3.923969  26.280847  \n",
       "12  3.443615  21.401905  \n",
       "10  3.088612  17.686726  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Identifying the numerical columns\n",
    "# with Skewness > 3.00 or Skewness < -3.00\n",
    "##################################\n",
    "display(numeric_column_quality_summary[(numeric_column_quality_summary['Skewness']>3) | (numeric_column_quality_summary['Skewness']<(-3))].sort_values(by=['Skewness'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2f99850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Formulating the dataset\n",
    "# with categorical columns only\n",
    "##################################\n",
    "thyroid_cancer_categorical = breast_cancer.select_dtypes(include=['category','object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "22039da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the variable names for the categorical column\n",
    "##################################\n",
    "categorical_variable_name_list = thyroid_cancer_categorical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "37f79644",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the first mode values for each categorical column\n",
    "##################################\n",
    "categorical_first_mode_list = [breast_cancer[x].value_counts().index.tolist()[0] for x in thyroid_cancer_categorical]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "28e63df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the second mode values for each categorical column\n",
    "##################################\n",
    "categorical_second_mode_list = [breast_cancer[x].value_counts().index.tolist()[1] for x in thyroid_cancer_categorical]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f33e8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of first mode values for each categorical column\n",
    "##################################\n",
    "categorical_first_mode_count_list = [thyroid_cancer_categorical[x].isin([breast_cancer[x].value_counts(dropna=True).index.tolist()[0]]).sum() for x in thyroid_cancer_categorical]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4c56a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of second mode values for each categorical column\n",
    "##################################\n",
    "categorical_second_mode_count_list = [thyroid_cancer_categorical[x].isin([breast_cancer[x].value_counts(dropna=True).index.tolist()[1]]).sum() for x in thyroid_cancer_categorical]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5cec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the first mode to second mode ratio for each categorical column\n",
    "##################################\n",
    "categorical_first_second_mode_ratio_list = map(truediv, categorical_first_mode_count_list, categorical_second_mode_count_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f999c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the count of unique values for each categorical column\n",
    "##################################\n",
    "categorical_unique_count_list = thyroid_cancer_categorical.nunique(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0bc12bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the number of observations for each categorical column\n",
    "##################################\n",
    "categorical_row_count_list = list([len(thyroid_cancer_categorical)] * len(thyroid_cancer_categorical.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "63ac8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Gathering the unique to count ratio for each categorical column\n",
    "##################################\n",
    "categorical_unique_count_ratio_list = map(truediv, categorical_unique_count_list, categorical_row_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9052677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorical.Column.Name</th>\n",
       "      <th>First.Mode</th>\n",
       "      <th>Second.Mode</th>\n",
       "      <th>First.Mode.Count</th>\n",
       "      <th>Second.Mode.Count</th>\n",
       "      <th>First.Second.Mode.Ratio</th>\n",
       "      <th>Unique.Count</th>\n",
       "      <th>Row.Count</th>\n",
       "      <th>Unique.Count.Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diagnosis</td>\n",
       "      <td>B</td>\n",
       "      <td>M</td>\n",
       "      <td>357</td>\n",
       "      <td>212</td>\n",
       "      <td>1.683962</td>\n",
       "      <td>2</td>\n",
       "      <td>569</td>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categorical.Column.Name First.Mode Second.Mode  First.Mode.Count  \\\n",
       "0               diagnosis          B           M               357   \n",
       "\n",
       "   Second.Mode.Count  First.Second.Mode.Ratio  Unique.Count  Row.Count  \\\n",
       "0                212                 1.683962             2        569   \n",
       "\n",
       "   Unique.Count.Ratio  \n",
       "0            0.003515  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################\n",
    "# Generating a column quality summary for the categorical columns\n",
    "##################################\n",
    "categorical_column_quality_summary = pd.DataFrame(zip(categorical_variable_name_list,\n",
    "                                                    categorical_first_mode_list,\n",
    "                                                    categorical_second_mode_list,\n",
    "                                                    categorical_first_mode_count_list,\n",
    "                                                    categorical_second_mode_count_list,\n",
    "                                                    categorical_first_second_mode_ratio_list,\n",
    "                                                    categorical_unique_count_list,\n",
    "                                                    categorical_row_count_list,\n",
    "                                                    categorical_unique_count_ratio_list), \n",
    "                                        columns=['Categorical.Column.Name',\n",
    "                                                 'First.Mode',\n",
    "                                                 'Second.Mode',\n",
    "                                                 'First.Mode.Count',\n",
    "                                                 'Second.Mode.Count',\n",
    "                                                 'First.Second.Mode.Ratio',\n",
    "                                                 'Unique.Count',\n",
    "                                                 'Row.Count',\n",
    "                                                 'Unique.Count.Ratio'])\n",
    "display(categorical_column_quality_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d78c8652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of categorical columns\n",
    "# with First.Second.Mode.Ratio > 5.00\n",
    "##################################\n",
    "len(categorical_column_quality_summary[(categorical_column_quality_summary['First.Second.Mode.Ratio']>5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5e8c623b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# Counting the number of categorical columns\n",
    "# with Unique.Count.Ratio > 10.00\n",
    "##################################\n",
    "len(categorical_column_quality_summary[(categorical_column_quality_summary['Unique.Count.Ratio']>10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e0d2e",
   "metadata": {},
   "source": [
    "## 1.4. Data Preprocessing <a class=\"anchor\" id=\"1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a1e51",
   "metadata": {},
   "source": [
    "### 1.4.1 Data Splitting<a class=\"anchor\" id=\"1.4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03795e18",
   "metadata": {},
   "source": [
    "### 1.4.2 Outlier and Distributional Shape Analysis<a class=\"anchor\" id=\"1.4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c786a5d",
   "metadata": {},
   "source": [
    "### 1.4.3 Collinearity<a class=\"anchor\" id=\"1.4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d49982",
   "metadata": {},
   "source": [
    "## 1.5. Data Exploration <a class=\"anchor\" id=\"1.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a14ef",
   "metadata": {},
   "source": [
    "### 1.5.1 Exploratory Data Analysis<a class=\"anchor\" id=\"1.5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7faee",
   "metadata": {},
   "source": [
    "### 1.5.2 Hypothesis Testing<a class=\"anchor\" id=\"1.5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8f7fd",
   "metadata": {},
   "source": [
    "## 1.6. Premodelling Data Preparation <a class=\"anchor\" id=\"1.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd580a",
   "metadata": {},
   "source": [
    "### 1.6.1 Preprocessed Data Description<a class=\"anchor\" id=\"1.6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb9686",
   "metadata": {},
   "source": [
    "### 1.6.2 Preprocessing Pipeline Development<a class=\"anchor\" id=\"1.6.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef1a55",
   "metadata": {},
   "source": [
    "## 1.7. Model Development and Validation <a class=\"anchor\" id=\"1.7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e2ea8",
   "metadata": {},
   "source": [
    "### 1.7.1 Random Forest<a class=\"anchor\" id=\"1.7.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac74cee",
   "metadata": {},
   "source": [
    "### 1.7.2 AdaBoost<a class=\"anchor\" id=\"1.7.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8c374",
   "metadata": {},
   "source": [
    "### 1.7.3 Gradient Boosting<a class=\"anchor\" id=\"1.7.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4212b",
   "metadata": {},
   "source": [
    "### 1.7.4 XGBoost<a class=\"anchor\" id=\"1.7.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259b256",
   "metadata": {},
   "source": [
    "### 1.7.5 Light GBM<a class=\"anchor\" id=\"1.7.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88d328",
   "metadata": {},
   "source": [
    "### 1.7.6 CatBoost<a class=\"anchor\" id=\"1.7.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7ede6",
   "metadata": {},
   "source": [
    "## 1.8. Model Monitoring using the NannyML Framework <a class=\"anchor\" id=\"1.8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed768c9",
   "metadata": {},
   "source": [
    "### 1.8.1 Baseline Control<a class=\"anchor\" id=\"1.8.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bbd8e",
   "metadata": {},
   "source": [
    "### 1.8.2 Simulated Covariate Drift<a class=\"anchor\" id=\"1.8.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d836c3",
   "metadata": {},
   "source": [
    "### 1.8.3 Simulated Prior Shift<a class=\"anchor\" id=\"1.8.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd311ef",
   "metadata": {},
   "source": [
    "### 1.8.4 Simulated Concept Drift<a class=\"anchor\" id=\"1.8.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53250f",
   "metadata": {},
   "source": [
    "### 1.8.5 Simulated Missingness Spike<a class=\"anchor\" id=\"1.8.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcfcb5",
   "metadata": {},
   "source": [
    "### 1.8.6 Simulated Seasonal Pattern<a class=\"anchor\" id=\"1.8.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162441d",
   "metadata": {},
   "source": [
    "## 1.9. Consolidated Findings <a class=\"anchor\" id=\"1.9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a94785",
   "metadata": {},
   "source": [
    "# 2. Summary <a class=\"anchor\" id=\"Summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a14f94",
   "metadata": {},
   "source": [
    "# 3. References <a class=\"anchor\" id=\"References\"></a>\n",
    "\n",
    "* **[Book]** [Reliable Machine Learning](https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218/) by Cathy Chen, Niall Richard Murphy, Kranti Parisa, D. Sculley and Todd Underwood\n",
    "* **[Book]** [Designing Machine Learning Systems](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/) by Chip Huyen\n",
    "* **[Book]** [Machine Learning Design Patterns](https://www.oreilly.com/library/view/machine-learning-design/9781098115777/) by Valliappa Lakshmanan, Sara Robinson and Michael Munn\n",
    "* **[Book]** [Machine Learning Engineering](https://www.mlebook.com/wiki/doku.php) by Andriy Burkov\n",
    "* **[Book]** [Engineering MLOps](https://www.oreilly.com/library/view/engineering-mlops/9781800562882/) by Emmanuel Raj\n",
    "* **[Book]** [Introducing MLOps](https://www.oreilly.com/library/view/introducing-mlops/9781492083283/) by Mark Treveil, Nicolas Omont, Clément Stenac, Kenji Lefevre, Du Phan, Joachim Zentici, Adrien Lavoillotte, Makoto Miyazaki and Lynn Heidmann\n",
    "* **[Book]** [Practical MLOps](https://www.oreilly.com/library/view/practical-mlops/9781098103002/) by Noah Gift and Alfredo Deza\n",
    "* **[Book]** [Data Science on AWS](https://www.oreilly.com/library/view/data-science-on/9781492079385/) by Chris Fregly and Antje Barth\n",
    "* **[Book]** [Ensemble Methods for Machine Learning](https://www.manning.com/books/ensemble-methods-for-machine-learning) by Gautam Kunapuli\n",
    "* **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson\n",
    "* **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani\n",
    "* **[Book]** [Ensemble Methods: Foundations and Algorithms](https://www.taylorfrancis.com/books/mono/10.1201/b12207/ensemble-methods-zhi-hua-zhou) by Zhi-Hua Zhou\n",
    "* **[Book]** [Effective XGBoost: Optimizing, Tuning, Understanding, and Deploying Classification Models (Treading on Python)](https://www.taylorfrancis.com/books/mono/10.1201/b12207/ensemble-methods-zhi-hua-zhou) by Matt Harrison, Edward Krueger, Alex Rook, Ronald Legere and Bojan Tunguz\n",
    "* **[Python Library API]** [nannyML](https://www.nannyml.com/) by NannyML Team\n",
    "* **[Python Library API]** [NumPy](https://numpy.org/doc/) by NumPy Team\n",
    "* **[Python Library API]** [pandas](https://pandas.pydata.org/docs/) by Pandas Team\n",
    "* **[Python Library API]** [seaborn](https://seaborn.pydata.org/) by Seaborn Team\n",
    "* **[Python Library API]** [matplotlib.pyplot](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) by MatPlotLib Team\n",
    "* **[Python Library API]** [itertools](https://docs.python.org/3/library/itertools.html) by Python Team\n",
    "* **[Python Library API]** [sklearn.experimental](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.experimental) by Scikit-Learn Team\n",
    "* **[Python Library API]** [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) by Scikit-Learn Team\n",
    "* **[Python Library API]** [scipy](https://docs.scipy.org/doc/scipy/) by SciPy Team\n",
    "* **[Python Library API]** [sklearn.tree](https://scikit-learn.org/stable/modules/tree.html) by Scikit-Learn Team\n",
    "* **[Python Library API]** [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html) by Scikit-Learn Team\n",
    "* **[Python Library API]** [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) by Scikit-Learn Team\n",
    "* **[Python Library API]** [xgboost](https://xgboost.readthedocs.io/en/stable/python/index.html) by XGBoost Team\n",
    "* **[Python Library API]** [lightgbm](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) by LightGBM Team\n",
    "* **[Python Library API]** [catboost](https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier) by CatBoost Team\n",
    "* **[Python Library API]** [StatsModels](https://www.statsmodels.org/stable/index.html) by StatsModels Team\n",
    "* **[Python Library API]** [SciPy](https://scipy.org/) by SciPy Team\n",
    "* **[Article]** [Comprehensive Comparison of ML Model Monitoring Tools: Evidently AI, Alibi Detect, NannyML, WhyLabs, and Fiddler AI](https://medium.com/@tanish.kandivlikar1412/comprehensive-comparison-of-ml-model-monitoring-tools-evidently-ai-alibi-detect-nannyml-a016d7dd8219) by Tanish Kandivlikar (Medium)\n",
    "* **[Article]** [Monitoring AI in Production: Introduction to NannyML](https://adnankarol.medium.com/monitoring-ai-in-production-75f1260260cb) by Adnan Karol (Medium)\n",
    "* **[Article]** [Data Drift Explainability: Interpretable Shift Detection with NannyML](https://towardsdatascience.com/data-drift-explainability-interpretable-shift-detection-with-nannyml-83421319d05f/) by Marco Cerliani (Towards Data Science)\n",
    "* **[Article]** [An End-to-End ML Model Monitoring Workflow with NannyML in Python](https://www.datacamp.com/tutorial/model-monitoring-with-nannyml-in-python) by Bex Tuychiyev (DataCamp)\n",
    "* **[Article]** [Detecting Concept Drift: Impact on Machine Learning Performance](https://www.nannyml.com/blog/concept-drift) by Michal Oleszak (NannyML.Com)\n",
    "* **[Article]** [Estimating Model Performance Without Labels](https://www.nannyml.com/blog/machine-learning-performance-estimation) by Jakub Białek (NannyML.Com)\n",
    "* **[Article]** [Monitoring Workflow for Machine Learning Systems](https://www.nannyml.com/blog/machine-learning-monitoring-workflow) by Santiago Víquez (NannyML.Com)\n",
    "* **[Article]** [Don’t Let Yourself Be Fooled by Data Drift](https://www.nannyml.com/blog/when-data-drift-does-not-affect-performance-machine-learning-models) by Santiago Víquez (NannyML.Com)\n",
    "* **[Article]** [Understanding Data Drift: Impact on Machine Learning Model Performance](https://www.nannyml.com/blog/types-of-data-shift) by Jakub Białek (NannyML.Com)\n",
    "* **[Article]** [NannyML’s Guide to Data Quality and Covariate Shift](https://www.nannyml.com/blog/guide-data-quality-covariate-shift) by Magdalena Kowalczuk (NannyML.Com)\n",
    "* **[Article]** [From Reactive to Proactive: Shift your ML Monitoring Approach](https://www.nannyml.com/blog/proactive-ml-monitoring-workflow) by Qiamo (Luca) Zheng (NannyML.Com)\n",
    "* **[Article]** [How to Detect Under-Performing Segments in ML Models](https://www.nannyml.com/blog/detecting-underpeforming--segments-in-ml-models) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Building Custom Metrics for Predictive Maintenance](https://www.nannyml.com/blog/custom-metrics-predictive-maintenance) by Kavita Rana(NannyML.Com)\n",
    "* **[Article]** [3 Custom Metrics for Your Forecasting Models](https://www.nannyml.com/blog/custom-metrics-for-demand-forecasting-models) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [There's Data Drift, But Does It Matter?](https://www.nannyml.com/blog/data-drift-does-it-matter) by Santiago Víquez (NannyML.Com)\n",
    "* **[Article]** [Monitoring Custom Metrics without Ground Truth](https://www.nannyml.com/blog/monitoring-custom-metrics-without-ground-truth) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Which Multivariate Drift Detection Method Is Right for You: Comparing DRE and DC](https://www.nannyml.com/blog/dre-vs-dc) by Miles Weberman (NannyML.Com)\n",
    "* **[Article]** [Prevent Failure of Product Defect Detection Models: A Post-Deployment Guide](https://www.nannyml.com/blog/prevent-failure-of-product-defect-detection-models) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Common Pitfalls in Monitoring Default Prediction Models and How to Fix Them](https://www.nannyml.com/blog/monitor-default-prediction-models) by Miles Weberman (NannyML.Com)\n",
    "* **[Article]** [Why Relying on Training Data for ML Monitoring Can Trick You](https://www.nannyml.com/blog/choose-reference-dataset) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Estimating Model Performance Without Labels](https://www.nannyml.com/blog/machine-learning-performance-estimation) by Jakub Białek (NannyML.Com)\n",
    "* **[Article]** [Using Concept Drift as a Model Retraining Trigger](https://www.nannyml.com/blog/concept-drift-retraining-trigger) by Taliya Weinstein (NannyML.Com)\n",
    "* **[Article]** [Retraining is Not All You Need](https://www.nannyml.com/blog/retraining-is-not-all-you-need) by Miles Weberman (NannyML.Com)\n",
    "* **[Article]** [A Comprehensive Guide to Univariate Drift Detection Methods](https://www.nannyml.com/blog/comprehensive-guide-univariate-methods) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Stress-free Monitoring of Predictive Maintenance Models](https://www.nannyml.com/blog/monitor-predictive-maintenance-models-stress-free) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Effective ML Monitoring: A Hands-on Example](https://www.nannyml.com/blog/ml-monitoring-workflow-hands-on) by Miles Weberman (NannyML.Com)\n",
    "* **[Article]** [Don’t Drift Away with Your Data: Monitoring Data Drift from Setup to Cloud](https://www.nannyml.com/blog/monitoring-data-drift) by Taliya Weinstein (NannyML.Com)\n",
    "* **[Article]** [Comparing Multivariate Drift Detection Algorithms on Real-World Data](https://www.nannyml.com/blog/tutorial-multivariate-drift-comparison) by Kavita Rana (NannyML.Com)\n",
    "* **[Article]** [Detect Data Drift Using Domain Classifier in Python](https://www.nannyml.com/blog/data-drift-domain-classifier) by Miles Weberman (NannyML.Com)\n",
    "* **[Article]** [Guide: How to evaluate if NannyML is the right monitoring tool for you](https://www.nannyml.com/blog/evaluate-nannyml) by Santiago Víquez (NannyML.Com)\n",
    "* **[Article]** [How To Monitor ML models with NannyML SageMaker Algorithms](https://www.nannyml.com/blog/how-to-monitor-ml-models-with-nannyml-sagemaker-algorithms) by Wiljan Cools (NannyML.Com)\n",
    "* **[Article]** [Tutorial: Monitoring Missing and Unseen values with NannyML](https://www.nannyml.com/blog/monitoring-missing-values-tutorial) by Santiago Víquez (NannyML.Com)\n",
    "* **[Article]** [Monitoring Machine Learning Models: A Fundamental Practice for Data Scientists and Machine Learning Engineers](https://medium.com/data-science/monitoring-machine-learning-models-a-tried-and-true-cure-for-a-data-scientists-insomnia-c45b0979a878) by Saurav Pawar (Medium)\n",
    "* **[Article]** [Failure Is Not an Option: How to Prevent Your ML Model From Degradation](https://medium.com/nannyml/failure-is-not-an-option-how-to-prevent-your-ml-model-from-degradation-nannyml-4473898af34a) by Maciej Balawejder (Medium)\n",
    "* **[Article]** [Managing Data Drift and Data Distribution Shifts in the MLOps Lifecycle for Machine Learning Models](https://abhishek-reddy.medium.com/detecting-and-managing-data-distribution-shifts-in-the-mlops-lifecycle-for-machine-learning-models-1ea33ce84c3c) by Abhishek Reddy (Medium)\n",
    "* **[Article]** [“You Can’t Predict the Errors of Your Model”… Or Can You?](https://medium.com/data-science/you-cant-predict-the-errors-of-your-model-or-can-you-1a2e4a1f38a0) by Samuele Mazzanti (Medium)\n",
    "* **[Article]** [Understanding Concept Drift: A Simple Guide](https://medium.com/data-science/understanding-concept-drift-a-simple-guide-b2cf4e09deae) by Vitor Cerqueira (Medium)\n",
    "* **[Article]** [Detecting Covariate Shift: A Guide to the Multivariate Approach](https://medium.com/data-science/detecting-covariate-shift-a-guide-to-the-multivariate-approach-c099bd1891b9) by Michał Oleszak (Medium)\n",
    "* **[Article]** [Data Drift vs. Concept Drift: Differences and How to Detect and Address Them](https://dataheroes.ai/blog/data-drift-vs-concept-drift/) by DataHeroes Team (DataHeroes.AI)\n",
    "* **[Article]** [An Introduction to Machine Learning Engineering for Production /MLOps — Concept and Data Drifts](https://medium.com/data-science/an-introduction-to-machine-learning-engineering-for-production-part-1-2247bbca8a61) by Praatibh Surana (Medium)\n",
    "* **[Article]** [Concept Drift and Model Decay in Machine Learning](https://medium.com/data-science/concept-drift-and-model-decay-in-machine-learning-a98a809ea8d4) by Ashok Chilakapati (Medium)\n",
    "* **[Article]** [Data Drift: Types of Data Drift](https://medium.com/data-science/data-drift-part-1-types-of-data-drift-16b3eb175006) by Numal Jayawardena (Medium)\n",
    "* **[Article]** [Monitoring Machine Learning models](https://medium.com/data-science/monitoring-machine-learning-models-62d5833c7ecc) by Jacques Verre (Medium)\n",
    "* **[Article]** [Data drift: It Can Come At You From Anywhere](https://medium.com/data-science/data-drift-it-can-come-at-you-from-anywhere-b78eb186855) by Tirthajyoti Sarkar (Medium)\n",
    "* **[Article]** [Drift in Machine Learning](https://medium.com/data-science/drift-in-machine-learning-e49df46803a) by Piotr (Peter) Mardziel (Medium)\n",
    "* **[Article]** [Understanding Dataset Shift](https://medium.com/data-science/understanding-dataset-shift-f2a5a262a766) by Matthew Stewart (Medium)\n",
    "* **[Article]** [Calculating Data Drift in Machine Learning using Python](https://medium.com/data-science/calculating-data-drift-in-machine-learning-53676ff5646b) by Vatsal (Medium)\n",
    "* **[Article]** [91% of ML Models Degrade in Time](https://medium.com/data-science/91-of-ml-models-degrade-in-time-cfd467905615) by Santiago Víquez (Medium)\n",
    "* **[Article]** [Model Drift in Machine Learning](https://medium.com/data-science/model-drift-in-machine-learning-8023e3d08217) by Kurtis Pykes (Medium)\n",
    "* **[Article]** [Production Machine Learning Monitoring: Outliers, Drift, Explainers & Statistical Performance](https://medium.com/data-science/production-machine-learning-monitoring-outliers-drift-explainers-statistical-performance-d9b1d02ac158) by Alejandro Saucedo (Medium)\n",
    "* **[Article]** [How to Detect Model Drift in MLOps Monitoring](https://medium.com/data-science/how-to-detect-model-drift-in-mlops-monitoring-7a039c22eaf9) by Amit Paka (Medium)\n",
    "* **[Article]** [“My data drifted. What’s next?” How to handle ML model drift in production.](https://medium.com/data-science/my-data-drifted-whats-next-how-to-handle-ml-model-drift-in-production-78719ef007b1) by Elena Samuylova (Medium)\n",
    "* **[Article]** [Machine Learning Model Drift](https://medium.com/data-science/machine-learning-model-drift-9cc43ad530d6) by Sophia Yang (Medium)\n",
    "* **[Article]** [Estimating the Performance of an ML Model in the Absence of Ground Truth](https://medium.com/data-science/estimating-the-performance-of-an-ml-model-in-the-absence-of-ground-truth-cc87dbf6e57) by Eryk Lewinson (Medium)\n",
    "* **[Article]** [Ensemble: Boosting, Bagging, and Stacking Machine Learning](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) by Jason Brownlee (MachineLearningMastery.Com)\n",
    "* **[Article]** [Stacking Machine Learning: Everything You Need to Know](https://www.machinelearningpro.org/stacking-machine-learning/) by Ada Parker (MachineLearningPro.Org)\n",
    "* **[Article]** [Ensemble Learning: Bagging, Boosting and Stacking](https://duchesnay.github.io/pystatsml/machine_learning/ensemble_learning.html) by Edouard Duchesnay, Tommy Lofstedt and Feki Younes (Duchesnay.GitHub.IO)\n",
    "* **[Article]** [Stack Machine Learning Models: Get Better Results](https://developer.ibm.com/articles/stack-machine-learning-models-get-better-results/) by Casper Hansen (Developer.IBM.Com)\n",
    "* **[Article]** [GradientBoosting vs AdaBoost vs XGBoost vs CatBoost vs LightGBM](https://www.geeksforgeeks.org/gradientboosting-vs-adaboost-vs-xgboost-vs-catboost-vs-lightgbm/) by Geeks for Geeks Team (GeeksForGeeks.Org)\n",
    "* **[Article]** [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) by Jason Brownlee (MachineLearningMastery.Com)\n",
    "* **[Article]** [The Ultimate Guide to AdaBoost Algorithm | What is AdaBoost Algorithm?](https://www.mygreatlearning.com/blog/adaboost-algorithm/) by Ashish Kumar (MyGreatLearning.Com)\n",
    "* **[Article]** [A Gentle Introduction to Ensemble Learning Algorithms](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/) by Jason Brownlee (MachineLearningMastery.Com)\n",
    "* **[Article]** [Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning) by Necati Demir (Toptal.Com)\n",
    "* **[Article]** [The Essential Guide to Ensemble Learning](https://www.v7labs.com/blog/ensemble-learning-guide) by Rohit Kundu (V7Labs.Com)\n",
    "* **[Article]** [Develop an Intuition for How Ensemble Learning Works](https://machinelearningmastery.com/how-ensemble-learning-works/) by by Jason Brownlee (Machine Learning Mastery)\n",
    "* **[Article]** [Mastering Ensemble Techniques in Machine Learning: Bagging, Boosting, Bayes Optimal Classifier, and Stacking](https://rahuljain788.medium.com/mastering-ensemble-techniques-in-machine-learning-bagging-boosting-bayes-optimal-classifier-and-c1dd8052f53f) by Rahul Jain (Medium)\n",
    "* **[Article]** [Ensemble Learning: Bagging, Boosting, Stacking](https://ai.plainenglish.io/ml-tutorial-19-ensemble-learning-bagging-boosting-stacking-5a926db20ec5) by Ayşe Kübra Kuyucu (Medium)\n",
    "* **[Article]** [Ensemble: Boosting, Bagging, and Stacking Machine Learning](https://medium.com/@senozanAleyna/ensemble-boosting-bagging-and-stacking-machine-learning-6a09c31breast_cancer778) by Aleyna Şenozan (Medium)\n",
    "* **[Article]** [Boosting, Stacking, and Bagging for Ensemble Models for Time Series Analysis with Python](https://medium.com/@kylejones_47003/boosting-stacking-and-bagging-for-ensemble-models-for-time-series-analysis-with-python-d74ab9026782) by Kyle Jones (Medium)\n",
    "* **[Article]** [Different types of Ensemble Techniques — Bagging, Boosting, Stacking, Voting, Blending](https://medium.com/@abhishekjainindore24/different-types-of-ensemble-techniques-bagging-boosting-stacking-voting-blending-b04355a03c93) by Abhishek Jain (Medium)\n",
    "* **[Article]** [Mastering Ensemble Techniques in Machine Learning: Bagging, Boosting, Bayes Optimal Classifier, and Stacking](https://rahuljain788.medium.com/mastering-ensemble-techniques-in-machine-learning-bagging-boosting-bayes-optimal-classifier-and-c1dd8052f53f) by Rahul Jain (Medium)\n",
    "* **[Article]** [Understanding Ensemble Methods: Bagging, Boosting, and Stacking](https://divyabhagat.medium.com/understanding-ensemble-methods-bagging-boosting-and-stacking-7683c493ac19) by Divya bhagat (Medium)\n",
    "* **[Video Tutorial]** [Concept Drift Detection with NannyML | Webinar](https://www.youtube.com/watch?v=kBTty6JTW9Q) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Fooled by Data Drift: How to Monitor ML Without False Positives](https://www.youtube.com/watch?v=71vlPelFVs0) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Monitoring Custom Metrics Without Access to Targets](https://www.youtube.com/watch?v=tqPoaA0STHs) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Analyzing Your Model's Performance in Production](https://www.youtube.com/watch?v=qDufIIduw5M) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [How to Monitor Predictive Maintenance Models | Webinar Replay](https://www.youtube.com/watch?v=VuEYjEKUkWA) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Machine Learning Monitoring Workflow [Webinar]](https://www.youtube.com/watch?v=aXyc9TSl_u8) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Monitoring Machine Learning Models on AWS | Webinar](https://www.youtube.com/watch?v=82NAJG8m5r0) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Root Cause Analysis for ML Model Failure](https://www.youtube.com/watch?v=f-15Ri8MFAM) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Quantifying the Impact of Data Drift on Machine Learning Model Performance | Webinar](https://www.youtube.com/watch?v=pZJQCxt0aus) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [How to Detect Drift and Resolve Issues in Your Machine Learning Models?](https://www.youtube.com/watch?v=zkWDb2URdIQ) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Notebooks to Containers: Setting up Continuous (ML) Model Monitoring in Production](https://www.youtube.com/watch?v=00MJ-gvpRkI) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Performance Estimation using NannyML | Tutorial in Jupyter Notebook](https://www.youtube.com/watch?v=fo0ejdQFcT0) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [What Is NannyML? Introducing Our Open Source Python Library](https://www.youtube.com/watch?v=HgZm2JdE5Fo) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [How to Automatically Retrain Your Models with Concept Drift Detection?](https://www.youtube.com/watch?v=QoymE--4sPM) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [How to Use NannyML? Two Modes of Running Our Library](https://www.youtube.com/watch?v=ATKJXo6lTls) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [How to Integrate NannyML in Production? | Tutorial](https://www.youtube.com/watch?v=zdiM2ZFD__w) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Bringing Your Machine Learning Model to Production | Overview](https://www.youtube.com/watch?v=XH_4X6oo8hI) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [Notebooks to Containers: Setting Up Continuous (ML) Model Monitoring in Production](https://www.youtube.com/watch?v=00MJ-gvpRkI) by NannyML (YouTube)\n",
    "* **[Video Tutorial]** [ML Performance without Labels: Comparing Performance Estimation Methods (Webinar Replay)](https://www.youtube.com/watch?v=CKCSC9Srgnc) by NannyML (YouTube)\n",
    "* **[Course]** [DataCamp Python Data Analyst Certificate](https://app.datacamp.com/learn/career-tracks/data-analyst-with-python) by DataCamp Team (DataCamp)\n",
    "* **[Course]** [DataCamp Python Associate Data Scientist Certificate](https://app.datacamp.com/learn/career-tracks/associate-data-scientist-in-python) by DataCamp Team (DataCamp)\n",
    "* **[Course]** [DataCamp Python Data Scientist Certificate](https://app.datacamp.com/learn/career-tracks/data-scientist-in-python) by DataCamp Team (DataCamp)\n",
    "* **[Course]** [DataCamp Machine Learning Engineer Certificate](https://app.datacamp.com/learn/career-tracks/machine-learning-engineer) by DataCamp Team (DataCamp)\n",
    "* **[Course]** [DataCamp Machine Learning Scientist Certificate](https://app.datacamp.com/learn/career-tracks/machine-learning-scientist-with-python) by DataCamp Team (DataCamp)\n",
    "* **[Course]** [IBM Data Analyst Professional Certificate](https://www.coursera.org/professional-certificates/ibm-data-analyst) by IBM Team (Coursera)\n",
    "* **[Course]** [IBM Data Science Professional Certificate](https://www.coursera.org/professional-certificates/ibm-data-science) by IBM Team (Coursera)\n",
    "* **[Course]** [IBM Machine Learning Professional Certificate](https://www.coursera.org/professional-certificates/ibm-machine-learning) by IBM Team (Coursera)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlexplore_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
